{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING SENSOR DATA CLASS DIAGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](data/class_diagram_bsa.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import HiveContext\n",
    "import sys\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the sqlContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# Define the hive context\n",
    "hiveContext = HiveContext(sc)\n",
    "\n",
    "# Create the spark session.\n",
    "ss = pyspark.sql.SparkSession(sc)\n",
    "spark = ss.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Word Count\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "# Create sqlCtx object.\n",
    "# CSV are accessed as sql tables using this.\n",
    "sqlCtx = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API: getTime and plotResults\n",
    "\n",
    "- plotResults is the utility function that will plot the compressed and reconstructed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTime(x, dfTest):\n",
    "    return dfTest.at[int(x),'timeseries']\n",
    "\n",
    "def plotResults(dfs,plotTemplates):\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    ax.set_title('compression analysis')\n",
    "    linestyles = ['_', '-', '--', ':']\n",
    "    colors = ('b', 'g', 'r', 'c', 'm', 'y', 'k')\n",
    "    timeX=dfs[0]['timeseries'].tolist()\n",
    "    axes = [ax, ax.twinx()]\n",
    "    axes1Count=0\n",
    "    axes0Count=0\n",
    "    for i in xrange(len(plotTemplates)):\n",
    "        try:\n",
    "            print 2*i\n",
    "            ReconDF=dfs[2*i+1].to_frame(name='values')\n",
    "            ReconDF['time']=ReconDF.index\n",
    "            ReconDF=ReconDF.dropna()\n",
    "            ReconDF=ReconDF.sort(['time'], ascending=[1])\n",
    "            ReconDF['timeseries']=ReconDF.apply(lambda x: getTime(x['time'],dfs[2*i]), axis=1)\n",
    "            if(plotTemplates[i] in ['Actual Supply Flow','Occupied Command','Damper Position']):\n",
    "                axes1Count=1\n",
    "                axes[1].plot(dfs[2*i]['timeseries'].tolist(),dfs[2*i]['values'].tolist(),\n",
    "                             'k--',color=colors[i],label=plotTemplates[i])\n",
    "                axes[1].plot(ReconDF['timeseries'].tolist(),ReconDF['values'].tolist(),\n",
    "                             'k:',color=colors[i],label=plotTemplates[i]+'_reconstructed')\n",
    "            else:\n",
    "                axes0Count=1\n",
    "                axes[0].plot(dfs[2*i]['timeseries'].tolist(),dfs[2*i]['values'].tolist(),\n",
    "                             'k--',color=colors[i],label=plotTemplates[i])\n",
    "                axes[0].plot(ReconDF['timeseries'].tolist(),ReconDF['values'].tolist(),\n",
    "                             'k:',color=colors[i],label=plotTemplates[i]+'_reconstructed')\n",
    "        except:\n",
    "            print plotTemplates[i]\n",
    "    if(axes1Count==0):\n",
    "        print \"axes1Count\",axes1Count\n",
    "        print len(timeX)\n",
    "        axes[1].plot(timeX,[0 for x in timeX],'k',color='w',label='axes1')\n",
    "    if(axes0Count==0):\n",
    "        print \"axes0Count\",axes0Count\n",
    "        axes[0].plot(timeX,[0 for x in timeX],'k',color='w',label='axes0')\n",
    "    axes[0].legend(loc='upper left',fontsize='x-small')\n",
    "    axes[1].legend(loc='upper right',fontsize='x-small')\n",
    "    axes[1].set_ylabel('scale for top right legend')\n",
    "    axes[0].set_ylabel('scale for top left legend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS: encoder\n",
    "\n",
    "**encoder is the base class for the piecewise approiximation model.**\n",
    "\n",
    "1. This is the base class for the piecewise approximation models.\n",
    "2. **compress()** will compress the data.\n",
    "3. **recon()** will reconstruct the compressed data.\n",
    "4. **compute_error()** will compute the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder:\n",
    "    \"\"\"\n",
    "    The encoder/decoder class is the base class for all encoder/decoder pairs.\n",
    "    Subclasses encode different types of encoding.\n",
    "    EncoderLearner is a factory class for fitting encoders to data\n",
    "    \"\"\"\n",
    "    def __init__(self,raw,max_gap):\n",
    "        \"\"\"\n",
    "        given a spark DataFrame or Series (raw), find the best model of a given type\n",
    "        \"\"\"\n",
    "\n",
    "    def compress(self):\n",
    "        \"\"\"\n",
    "        given a raw sequence and a model, return a compressed representation.\n",
    "        \"\"\"\n",
    "        self.compressed=None\n",
    "        return self.compressed\n",
    "    \n",
    "    def recon(self,compressed):\n",
    "        \"\"\"\n",
    "        Recreate the original DataFrame or Series, possibly with errors.\n",
    "        \"\"\"\n",
    "        Recon=None\n",
    "        return Recon\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.compressed)\n",
    "    \n",
    "    def compute_error(self,S,compressed):\n",
    "        if type(compressed)==type(None):\n",
    "            compressed=self.compressed\n",
    "        #R=self.recon(compressed=compressed,index=S.index)\n",
    "        R=self.recon()\n",
    "        V=R-S\n",
    "        V=V.dropna()\n",
    "        return sqrt(sum([v*v for v in V.values]))/len(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS: piecewise_constant\n",
    "\n",
    "**piecewise_linear is the class that performs the piecewise constant approximation on the data. **\n",
    "\n",
    "1. This class inherits encoder class.\n",
    "2. compress and recon are overridden from base class.\n",
    "3. internal method fit() is used to fit the data.\n",
    "\n",
    "\n",
    "**Constructor:**\n",
    "\n",
    "- Takes input as series.\n",
    "- Calls fit() to train the model.\n",
    "\n",
    "**fit**\n",
    "\n",
    "- Takes series of values and max_gap as input.\n",
    "- Uses dynamic programming to create patches of minimized error.\n",
    "- Error is calculated iteratively by finding the error and number of switches.\n",
    "- values for minimum error is stored in each iteration.\n",
    "\n",
    "**recon**\n",
    "\n",
    "- Takes the array of time, value pairs and create a treashold points.\n",
    "- NaNs are removed and are not interpolated to have constant values.\n",
    "\n",
    "**Compress**\n",
    "\n",
    "- Creates an array of {time, value} values based on the current and previous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class piecewise_constant(encoder):\n",
    "    \"\"\"Represent the signal using a sequence of piecewise constant functions \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S, max_gap):\n",
    "        if type(S) != pd.Series:\n",
    "            raise 'encode expects pandas Series as input'\n",
    "        # Save the index and call the fit to update the model parameters.\n",
    "        self.index = S.index;\n",
    "        self.Sol = self.fit(S, max_gap)\n",
    "    \n",
    "    def fit(self, S, max_gap):\n",
    "        # Replace the nan values with 0\n",
    "        S[np.isnan(S)] = 0\n",
    "        \n",
    "        # Calculate the range of values.\n",
    "        # _range is a constant that is added to the error at each stop point\n",
    "        # Larger values will cause fewer switches.\n",
    "        _range=np.max(S) - np.min(S)\n",
    "        print 'range=', _range\n",
    "\n",
    "        # Dynamic programming.\n",
    "        # An array that holds the best partition ending at each point of the sequence.\n",
    "        # Each element contains a best current value, a pointer to the last change in best\n",
    "        # solution so far and the total error of best solution so far.\n",
    "        Sol = [[]] * len(S)\n",
    "        for i in range(len(S)):\n",
    "            if i == 0:\n",
    "                Sol[i] = {'prev' : None, 'value' : S[0], 'error' : 0.0, 'switch_no' : 0}\n",
    "            # Sol is indexed by the location in the sequence S\n",
    "            # prev: the index of the last switch point\n",
    "            # value: current prediction value\n",
    "            # error: cumulative error to this point\n",
    "            # switch_no: number of switches so far.\n",
    "            else:\n",
    "                # Calculate the squared error with previous value.\n",
    "                err0 = Sol[i-1]['error'] + (Sol[i-1]['value'] - S[i]) ** 2\n",
    "                best, best_err, best_val = None, 1e20, S[i]\n",
    "                for j in xrange(np.max([0, i - max_gap]), i):\n",
    "                    \n",
    "                    # Calculate the mean and standard deviation of gap.\n",
    "                    _mean, _std = np.mean(S[j : i]), np.std(S[j : i])\n",
    "                    \n",
    "                    # Calculate the error\n",
    "                    err = _std * (i - j) + Sol[j]['error'] + _range\n",
    "                    \n",
    "                    # Compare and get the best params\n",
    "                    if err < best_err:\n",
    "                        best, best_val, best_err = j, _mean, err\n",
    "                # Store the best params.\n",
    "                Sol[i] = {'prev' : best, 'value' : best_val, 'error' : best_err,\\\n",
    "                        'switch_no': Sol[best]['switch_no'] + 1}\n",
    "        return Sol\n",
    "\n",
    "    def compress(self, S):\n",
    "        \"\"\"Compress the data.\"\"\"\n",
    "        # Initiallize the switch points.\n",
    "        Switch_points = []\n",
    "\n",
    "        # start from the end \n",
    "        i = len(self.Sol) - 1\n",
    "\n",
    "        while i > 0:\n",
    "            prev, value = self.Sol[i]['prev'], self.Sol[i]['value']\n",
    "            if self.Sol[prev]['value'] != value:\n",
    "                Switch_points.append({'time':S.index[prev],'value':value})\n",
    "            i = prev\n",
    "        self.compressed = Switch_points\n",
    "        return Switch_points\n",
    "\n",
    "    def recon(self, compressed = None, index = None):\n",
    "        \"\"\"Reconstructs the data from compressed data\n",
    "        \"\"\"\n",
    "        if type(index) == type(None):\n",
    "            index = self.index\n",
    "        Recon = pd.Series(index=index)\n",
    "\n",
    "        if type(compressed) == type(None):\n",
    "            compressed = self.compressed\n",
    "        for e in compressed:\n",
    "            time = e['time']\n",
    "            Recon[time] = e['value']\n",
    "\n",
    "        return Recon.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS: piecewise_linear\n",
    "\n",
    "**piecewise_linear is the class that performs the piecewise linear approximation on the data. **\n",
    "\n",
    "1. This class inherits encoder class.\n",
    "2. compress and recon are overridden from base class.\n",
    "3. internal method fit() is used to fit the data.\n",
    "\n",
    "\n",
    "**Constructor:**\n",
    "- Takes input as series.\n",
    "- Calls fit() to train the model.\n",
    "\n",
    "**fit**\n",
    "- Takes series of values and max_gap as input.\n",
    "- Uses dynamic programming to create patches of minimized error.\n",
    "- Error is calculated iteratively by finding the slope for each max_gap.\n",
    "- values for minimum error is stored in each iteration.\n",
    "\n",
    "**recon**\n",
    "\n",
    "- Linear interpolation is performed for reconstruction from the compresseed data.\n",
    "- numpy interpolation function is used.\n",
    "\n",
    "**Compress**\n",
    "\n",
    "- Iteratively stores the time and values for every changing slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class piecewise_linear(encoder):\n",
    "    \"\"\" \n",
    "    Represent the signal using a sequence of piecewise linear functions \n",
    "    \"\"\"\n",
    "    def __init__(self, S, max_gap):\n",
    "        if type(S) != pd.Series:\n",
    "            raise 'encode expects pandas Series as input'\n",
    "        # Save the index and call the fit to update the model parameters.\n",
    "        self.index = S.index\n",
    "        self.Sol = self.fit(S, max_gap)\n",
    "    \n",
    "    # fit uses dynamic programming to find the best piecewise linear solution\n",
    "    # max_gap is the maximal extent of a single step.\n",
    "    # Reason for max_gap is that even if the error is small we want to correct\n",
    "    # it with some minimal frequence. \n",
    "    # Not quite a snapshot because the value will not necessarily change after \n",
    "    # max_gap is reached.\n",
    "    def fit(self,S,max_gap):\n",
    "        # Replace the nan values with 0\n",
    "        S[np.isnan(S)] = 0\n",
    "        \n",
    "        # Calculate the range of values.\n",
    "        # _range is a constant that is added to the error at each stop point\n",
    "        # Larger values will cause fewer switches.\n",
    "        _range=np.max(S) - np.min(S)\n",
    "        print 'range=', _range\n",
    "\n",
    "        # Dynamic programming.\n",
    "        # An array that holds the best partition ending at each point of the sequence.\n",
    "        # Each element contains a best current value, a pointer to the last change in best\n",
    "        # solution so far and the total error of best solution so far.\n",
    "        Sol = [[]] * len(S)\n",
    "        for i in range(len(S)):\n",
    "            if i == 0:\n",
    "                Sol[i]={'prev':None, 'value':S[0], 'error':0.0, 'switch_no':0, 'slope':0}\n",
    "            # Sol is indexed by the location in the sequence S\n",
    "            # prev: the index of the last switch point\n",
    "            # value: current prediction value\n",
    "            # error: cumulative error to this point\n",
    "            # slope: slope of th linear line at this point\n",
    "            else:\n",
    "                err0 = Sol[i-1]['error'] + (Sol[i-1]['value'] - S[i]) ** 2\n",
    "                best, best_err, best_val, best_slope = None, 1e20, S[i], 1e20\n",
    "                for j in xrange(np.max([0, i - max_gap]), i):\n",
    "                    \n",
    "                    # Calculate the slope\n",
    "                    _slope=(S[i] - S[j]) * 1.0 / (i - j)\n",
    "                    \n",
    "                    # Initialize the parameters.\n",
    "                    _val, _err = 0, 0\n",
    "                    for k in xrange(j, i):\n",
    "                        # Calculate the new value based on slope.\n",
    "                        _val = Sol[j]['value'] + _slope * (k - j)\n",
    "                        \n",
    "                        # Calculate the error.\n",
    "                        _err += (Sol[k]['value'] - _val) ** 2\n",
    "                    \n",
    "                    # Calculate the total error.\n",
    "                    # Need to understand why _range is addeed to error.\n",
    "                    err = _err * 1.0 / (i - j) + Sol[j]['error'] + _range\n",
    "                    _val = Sol[j]['value'] + _slope * (i - j)\n",
    "                    \n",
    "                    # Compare and get the best params.\n",
    "                    if err < best_err:\n",
    "                        best, best_val, best_err, best_slope, = j, _val,err,_slope\n",
    "\n",
    "                # Save the best params\n",
    "                Sol[i] = {'prev':best, 'value':best_val, 'error':best_err,\\\n",
    "                        'switch_no': Sol[best]['switch_no']+1, 'slope':best_slope}\n",
    "\n",
    "        # Return the fit parameters.\n",
    "        return Sol\n",
    "\n",
    "    def compress(self,S):\n",
    "        Switch_points = []\n",
    "        \n",
    "        # start from the end \n",
    "        i = len(self.Sol) - 1\n",
    "        while i > 0:\n",
    "            prev, slope, value, = self.Sol[i]['prev'], self.Sol[i]['slope'], self.Sol[i]['value']\n",
    "            if self.Sol[prev]['slope'] != slope:\n",
    "                Switch_points.append({'time' : S.index[prev], 'value' : value})\n",
    "            i = prev\n",
    "\n",
    "        # Save the compressed data and return the data.\n",
    "        self.compressed = Switch_points\n",
    "        return Switch_points\n",
    "\n",
    "    def recon(self,compressed=None, index=None):\n",
    "        if type(index)==type(None):\n",
    "            index = self.index\n",
    "\n",
    "        # Initialize the recon series.\n",
    "        Recon = pd.Series(index=index)\n",
    "\n",
    "        if type(compressed) == type(None):\n",
    "            compressed = self.compressed\n",
    "        for e in compressed:\n",
    "            time = e['time']\n",
    "            Recon[time] = e['value']\n",
    "        \n",
    "        # Interpolate the value using linear method.\n",
    "        Recon.interpolate(method=\"linear\", inplace=True)\n",
    "        return Recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API: model\n",
    "\n",
    "**model is the API that calls piecewise linear / piecewise constant based on method,\n",
    "compresses, recostructs and calculates the error.**\n",
    "\n",
    "1. Get the encoder object by calling piecewise_constant / piecewise_linear\n",
    "2. Compress the data.\n",
    "3. Reconstruct the data.\n",
    "4. Calculate the compression error.\n",
    "5. Returns the compressed and reconstructed data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(pd_df, method, tolerance):\n",
    "    \"\"\"model calls either piecewise_constant or piecewise_linear based on the method.\n",
    "    It gets the appropriate encoder, compresses, reconstructs the data and calculates the error.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the values\n",
    "    S = pd_df['values']\n",
    "    \n",
    "    # Calculate the standard deviation of the values.\n",
    "    _std = np.std(S)\n",
    "    print \"Std dev is \", _std\n",
    "    # Call piecewise_constant / piecewise_linear API based on method and get the encoder.\n",
    "    if method == 'piecewise_constant':\n",
    "        encoder = piecewise_constant(S, tolerance)\n",
    "    elif(method == 'piecewise_linear'):\n",
    "        encoder = piecewise_linear(S, tolerance) # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX chage it to linear\n",
    "    \n",
    "    # Compress the data as per the encoder.\n",
    "    C = encoder.compress(S)\n",
    "    \n",
    "    # Reconstruct the encoded data.\n",
    "    R = encoder.recon()\n",
    "\n",
    "    # Create the data frame of comparessed data.\n",
    "    compressed_df = pd.DataFrame(C)\n",
    "\n",
    "    # Calculate the error between compressed and original data.\n",
    "    error = encoder.compute_error(S, compressed=C)\n",
    "    \n",
    "    print 'error =', error, 'error/_std=',error/_std\n",
    "    \n",
    "    # Return the compressed and re constructed dataframes.\n",
    "    return [compressed_df, R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API: runAnalysis\n",
    "\n",
    "**runAnalysis is the API that performs piecewise linear / piecewise constant analysis on a given teamplate.**\n",
    "\n",
    "1. Create a query based on template.\n",
    "2. Project timeseries and values.\n",
    "3. Convert the data into appropriate datatypes.\n",
    "4. Call the model on the data.\n",
    "5. Returns the compressed and reconstructed data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runAnalysis(table, stTime, enTime, templateCount):\n",
    "    \"\"\"runAnalysis is the API that performs piecewise linear / piecewise constant analysis on a given teamplate.\"\"\"\n",
    "\n",
    "    # These are the available templates in the data\n",
    "    templates=['Zone Temperature', 'Actual Supply Flow', 'Occupied Clg Min', 'Occupied Htg Flow',\n",
    "               'Common Setpoint', 'Actual Heating Setpoint', 'Supply Vel Press', 'Zone Temperature Error',\n",
    "               'Damper Position', 'Warm Cool Adjust', 'Cooling Command', 'HVAC Zone Power',\n",
    "               'Damper Command', 'Cooling Max Flow', 'Occupied Htg Flow','Actual Cooling Setpoint',\n",
    "               'Reheat Valve Command Error']\n",
    "    \n",
    "    # Initialize the data frames and templates to plot.\n",
    "    dfs = []\n",
    "    plotTemplates=[]\n",
    "    \n",
    "    # Run the analysis for each template upto the template count.\n",
    "    for t in templates[0 : templateCount]:\n",
    "        \n",
    "        try:\n",
    "            # Create a query to fetch the data from the tabels.\n",
    "            # CSV data is read and is registered as tables.\n",
    "            query = \"SELECT * FROM \" + str(table) + \" WHERE template='\" + \\\n",
    "                str(t) + \"' AND timeseries BETWEEN '\" + str(stTime) + \"' AND '\" + str(enTime) + \"'\" \n",
    "\n",
    "            print query\n",
    "\n",
    "            # Execute the query. The output is a spark dataframe.\n",
    "            sparkDF = spark.sql(query)\n",
    "\n",
    "            # Project timeseries and values and store in pandas dataframe.\n",
    "            dataDF = pd.DataFrame(sparkDF.select('timeseries', 'values').collect(),\n",
    "                                  columns=['timeseries','values'])\n",
    "\n",
    "            # Convert timeseries string object into a datetime object.\n",
    "            dataDF['timeseries'] = dataDF['timeseries'].apply(\n",
    "                lambda x:datetime.strptime(x, '%Y-%m-%dT%H:%M:%S+00:00'))\n",
    "\n",
    "            # Convert values into float\n",
    "            dataDF['values'] = dataDF['values'].apply(lambda x : float(str(x)))\n",
    "\n",
    "            # Perform piecewise linear analysis if template is Zone Temperature or Zone Temperature Error\n",
    "            # Perform piecewise constant analysis if template is any other value.\n",
    "            if t in ['Zone Temperature','Zone Temperature Error']:\n",
    "                method = 'piecewise_linear'\n",
    "            else:\n",
    "                method = 'piecewise_constant'\n",
    "\n",
    "            # Run the model and get the compressed dataframe and reconstructed data frame.\n",
    "            [compressedDF, reconDF] = model(dataDF, method, tolerance = 96)\n",
    "\n",
    "            # Append the template and output data frames of model.\n",
    "            plotTemplates.append(t)\n",
    "            dfs.extend([dataDF, reconDF])\n",
    "        except:\n",
    "            print \"Exception for template: \", t\n",
    "\n",
    "    # Return the templates and dataframes.\n",
    "    return [dfs, plotTemplates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
