{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv('../ebu3b/data/ebu3b_metadata.csv')\n",
    "\n",
    "def get_df_metadata():\n",
    "    return df_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Weather data\n",
    "\n",
    "- Weather data for La Jolla is purchased from https://openweathermap.org/\n",
    "![weather description](weather_metadata.png)\n",
    "\n",
    "**2a. Weather data cleanup**\n",
    "\n",
    "- Remove columns that have all / most nans\n",
    "- Remove columns that are not applicable (city_id, weather_icon)\n",
    "\n",
    "columns that are retained are:\n",
    "\n",
    "1) dt_iso  \n",
    "2) temp  \n",
    "3) temp_min  \n",
    "4) temp_max  \n",
    "5) pressure  \n",
    "6) humidity  \n",
    "7) wind_speed  \n",
    "8) wind_deg  \n",
    "9) clouds_all  \n",
    "10) weather_id  \n",
    "11) weather_main\n",
    "\n",
    "**2b. Feature engineering**\n",
    "\n",
    "1) Conversion of temperature columns from Klevin to farenheit  \n",
    "2) Label encoding for weather_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns required from weather data\n",
    "req_columns = ['dt_iso', 'temp', 'temp_min', 'temp_max', 'pressure', 'humidity',\n",
    "               'wind_speed', 'wind_deg', 'clouds_all', 'weather_id', 'weather_main']\n",
    "# Read the weather data frame\n",
    "weather_df = pd.read_csv('../ebu3b/weather.csv')[req_columns]\n",
    "\n",
    "# Remove UTC and convert to datetime\n",
    "weather_df['dt_iso'] = pd.to_datetime(weather_df.dt_iso.str.replace(\" UTC\", \"\"))\n",
    "# Convert to pacific standard time.\n",
    "weather_df = weather_df.set_index('dt_iso').tz_localize('UTC').tz_convert('US/Pacific').reset_index()\n",
    "\n",
    "# Strip time zone info and convert to datetime.\n",
    "weather_df['dt_iso'] = pd.to_datetime(weather_df.dt_iso.astype('str').str.replace(\"-07:00\", \"\"))\n",
    "\n",
    "# Convert temperatures to farenheit\n",
    "weather_df['temp'] = weather_df.temp * 9 / 5 - 459.67\n",
    "weather_df['temp_min'] = weather_df.temp_min * 9 / 5 - 459.67\n",
    "weather_df['temp_max'] = weather_df.temp_max * 9 / 5 - 459.67\n",
    "\n",
    "# Encode the weather_main\n",
    "weather_df['weather_main'] = LabelEncoder().fit_transform(weather_df.weather_main)\n",
    "\n",
    "# Create datetime columns\n",
    "weather_df['date'] = weather_df.dt_iso.dt.date\n",
    "weather_df['hour'] = weather_df.dt_iso.dt.hour\n",
    "\n",
    "def get_df_weather():\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframe for the given signals\n",
    "\n",
    "- Takes the room, list of signals, mean_type, weather as input\n",
    "- if mean_type is hour it groups by hour and averages the value for each hour\n",
    "- If mean_type is quarter_hout it groups by every 15 minutes\n",
    "- If weather is True it combines with weather data or returns the raw data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../ebu3b/data/\"\n",
    "\n",
    "# Returns the bucket time based on original minutes and bucket_size\n",
    "def get_bucket(min_hour, bucket_size):\n",
    "    return (min_hour / bucket_size).astype(\"int\") * bucket_size\n",
    "\n",
    "def get_signal_dataframe(room, signals = None, mean_type=\"hour\", use_weather_data=True):\n",
    "    df_list = []\n",
    "    df_filtered = df_metadata[(df_metadata.Location == room)]\n",
    "    if not signals is None:\n",
    "        df_filtered = df_filtered[df_filtered['Ground Truth Point Type'].isin(signals)]\n",
    "\n",
    "    for identifier in df_filtered['Unique Identifier'].values:\n",
    "        # Filename is same as identifier\n",
    "        filename = identifier + \".csv\"\n",
    "        \n",
    "        # Read the csv \n",
    "        df = pd.read_csv(data_path + filename).dropna()\n",
    "        \n",
    "        # Convert to datetime object\n",
    "        df[\"time\"] = pd.to_datetime(df.time)\n",
    "\n",
    "        min_bucket_size = None\n",
    "        if mean_type == \"hour\":\n",
    "            min_bucket_size = 60\n",
    "        elif mean_type == \"quarter_hour\":\n",
    "            min_bucket_size = 15\n",
    "        elif mean_type == '5minutes':\n",
    "            min_bucket_size = 5\n",
    "\n",
    "        hour_bucket_size = None\n",
    "        if mean_type == 'day':\n",
    "            hour_bucket_size = 24\n",
    "        elif mean_type == 'quarter_day':\n",
    "            hour_bucket_size = 6\n",
    "\n",
    "        if not min_bucket_size is None:\n",
    "            hours = df.time.dt.hour.astype(\"str\") + \" hours\"\n",
    "            mins = get_bucket(df.time.dt.minute, min_bucket_size).astype(\"str\") + \" minutes\"\n",
    "            # Groupby hour and average the values per hour. This is to reduce the number of data points.\n",
    "            groupby_item = pd.to_datetime(df.time.dt.date) + pd.to_timedelta(hours + \" \" + mins)\n",
    "\n",
    "        if not hour_bucket_size is None:\n",
    "            hours = get_bucket(df.time.dt.hour, hour_bucket_size).astype(\"str\") + \" hours\"\n",
    "            # Groupby hour and average the values per hour. This is to reduce the number of data points.\n",
    "            groupby_item = pd.to_datetime(df.time.dt.date) + pd.to_timedelta(hours)\n",
    "        \n",
    "        if not mean_type is None:\n",
    "            df = df.groupby(groupby_item)[['value']].mean().reset_index()\n",
    "        # Create new colunms\n",
    "        df[\"identifier\"], df['location'] = identifier, room\n",
    "\n",
    "        # append the data frame to list\n",
    "        df_list.append(df)\n",
    "        \n",
    "        print \"Read file: \", filename\n",
    "\n",
    "    df_all = pd.concat(df_list)\n",
    "\n",
    "    # Merge the dataframe with meta data and filter hte required columns\n",
    "    df_all = df_all.merge(df_metadata, right_on=\"Unique Identifier\", left_on=\"identifier\")[[\"time\",\n",
    "        \"value\", \"identifier\", \"location\", \"Ground Truth Point Type\"]]\n",
    "    rm_signals = df_all.pivot_table(values='value', index=['time', 'location'], \\\n",
    "                                                  columns=\"Ground Truth Point Type\").reset_index()\n",
    "    if use_weather_data:\n",
    "        rm_signals = weather_df.merge(rm_signals, left_on='dt_iso', right_on='time')\n",
    "    return rm_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for the day\n",
    "\n",
    "- Takes input the data frame and the day for which model must be running\n",
    "- It runs the linear regression, Lasso, Ridge, DecisionTreeRegressor, AdaBoostRegressor on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_for_day(model_df, features, target, day='Sunday'):\n",
    "    model_df = model_df.dropna()\n",
    "    X = model_df[model_df.dt_iso.dt.weekday_name == day][features]\n",
    "    y = model_df[model_df.dt_iso.dt.weekday_name == day][target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "\n",
    "    list_df = []\n",
    "    reg = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "    list_df.append(pd.DataFrame({'model' : 'DecisionTreeRegressor', 'train score' : reg.score(X_train, y_train),\n",
    "                  'test score' : reg.score(X_test, y_test)}, index=[0]))\n",
    "\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    list_df.append(pd.DataFrame({'model' : 'LinearRegression', 'train score' : reg.score(X_train, y_train),\n",
    "                  'test score' : reg.score(X_test, y_test)}, index=[0]))\n",
    "\n",
    "    reg = Lasso().fit(X_train, y_train)\n",
    "    list_df.append(pd.DataFrame({'model' : 'Lasso', 'train score' : reg.score(X_train, y_train),\n",
    "                  'test score' : reg.score(X_test, y_test)}, index=[0]))\n",
    "\n",
    "    reg = Ridge().fit(X_train, y_train)\n",
    "    list_df.append(pd.DataFrame({'model' : 'Ridge', 'train score' : reg.score(X_train, y_train),\n",
    "                  'test score' : reg.score(X_test, y_test)}, index=[0]))\n",
    "\n",
    "    reg = AdaBoostRegressor().fit(X_train, y_train)\n",
    "    list_df.append(pd.DataFrame({'model' : 'AdaBoostRegressor', 'train score' : reg.score(X_train, y_train),\n",
    "                  'test score' : reg.score(X_test, y_test)}, index=[0]))\n",
    "\n",
    "    return pd.concat(list_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: getTime and plotResults\n",
    "\n",
    "- plotResults is the utility function that will plot the compressed and reconstructed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTime(x, dfTest):\n",
    "    return dfTest.at[int(x),'timeseries']\n",
    "\n",
    "def plotResults(dfs, plotTemplates, method, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.set_title('compression analysis for ' + method)\n",
    "    linestyles = ['_', '-', '--', ':']\n",
    "    colors = ('b', 'g', 'r', 'c', 'm', 'y', 'k')\n",
    "    timeX=dfs[0]['timeseries'].tolist()\n",
    "    axes = [ax, ax.twinx()]\n",
    "    axes1Count=0\n",
    "    axes0Count=0\n",
    "    for i in xrange(len(plotTemplates)):\n",
    "        try:\n",
    "            ReconDF=dfs[2*i+1].to_frame(name='values')\n",
    "            ReconDF['time']=ReconDF.index\n",
    "            ReconDF=ReconDF.dropna()\n",
    "            ReconDF=ReconDF.sort_values(by=['time'], ascending=[1])\n",
    "            ReconDF['timeseries']=ReconDF.apply(lambda x: getTime(x['time'],dfs[2*i]), axis=1)\n",
    "            if(plotTemplates[i] in ['Actual Supply Flow','Occupied Command','Damper Position']):\n",
    "                axes1Count=1\n",
    "                axes[1].plot(dfs[2*i]['timeseries'].tolist(),dfs[2*i]['values'].tolist(),\n",
    "                             'k--',color=colors[i],label=plotTemplates[i])\n",
    "                axes[1].plot(ReconDF['timeseries'].tolist(),ReconDF['values'].tolist(),\n",
    "                             'k:',color=colors[i],label=plotTemplates[i]+'_reconstructed')\n",
    "            else:\n",
    "                axes0Count=1\n",
    "                axes[0].plot(dfs[2*i]['timeseries'].tolist(),dfs[2*i]['values'].tolist(),\n",
    "                             'k--',color=colors[i],label=plotTemplates[i])\n",
    "                axes[0].plot(ReconDF['timeseries'].tolist(),ReconDF['values'].tolist(),\n",
    "                             'k:',color=colors[i],label=plotTemplates[i]+'_reconstructed')\n",
    "        except:\n",
    "            print \"Exception for : \", plotTemplates[i]\n",
    "    if(axes1Count==0):\n",
    "        axes[1].plot(timeX,[0 for x in timeX],'k',color='w',label='axes1')\n",
    "        axes[1].grid(True)\n",
    "    if(axes0Count==0):\n",
    "        axes[0].plot(timeX,[0 for x in timeX],'k',color='w',label='axes0')\n",
    "        axes[0].grid(True)\n",
    "    axes[0].legend(loc='upper left',fontsize='x-small')\n",
    "    axes[1].legend(loc='upper right',fontsize='x-small')\n",
    "    axes[1].set_ylabel('scale for top right legend')\n",
    "    axes[0].set_ylabel('scale for top left legend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS: encoder\n",
    "\n",
    "**encoder is the base class for the piecewise approiximation model.**\n",
    "\n",
    "1. This is the base class for the piecewise approximation models.\n",
    "2. **compress()** will compress the data.\n",
    "3. **recon()** will reconstruct the compressed data.\n",
    "4. **compute_error()** will compute the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder:\n",
    "    \"\"\"\n",
    "    The encoder/decoder class is the base class for all encoder/decoder pairs.\n",
    "    Subclasses encode different types of encoding.\n",
    "    EncoderLearner is a factory class for fitting encoders to data\n",
    "    \"\"\"\n",
    "    def __init__(self, raw, max_gap):\n",
    "        \"\"\"\n",
    "        given a spark DataFrame or Series (raw), find the best model of a given type\n",
    "        \"\"\"\n",
    "\n",
    "    def compress(self):\n",
    "        \"\"\"\n",
    "        given a raw sequence and a model, return a compressed representation.\n",
    "        \"\"\"\n",
    "        self.compressed=None\n",
    "        return self.compressed\n",
    "    \n",
    "    def recon(self,compressed):\n",
    "        \"\"\"\n",
    "        Recreate the original DataFrame or Series, possibly with errors.\n",
    "        \"\"\"\n",
    "        Recon=None\n",
    "        return Recon\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.compressed)\n",
    "    \n",
    "    def compute_error(self,S,compressed):\n",
    "        if type(compressed)==type(None):\n",
    "            compressed=self.compressed\n",
    "        #R=self.recon(compressed=compressed,index=S.index)\n",
    "        R=self.recon()\n",
    "        V=R-S\n",
    "        V=V.dropna()\n",
    "        return np.sqrt(sum([v*v for v in V.values]))/len(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS: piecewise_constant\n",
    "\n",
    "**piecewise_linear is the class that performs the piecewise constant approximation on the data. **\n",
    "\n",
    "1. This class inherits encoder class.\n",
    "2. compress and recon are overridden from base class.\n",
    "3. internal method fit() is used to fit the data.\n",
    "\n",
    "\n",
    "**Constructor:**\n",
    "\n",
    "- Takes input as series.\n",
    "- Calls fit() to train the model.\n",
    "\n",
    "**fit**\n",
    "\n",
    "- Takes series of values and max_gap as input.\n",
    "- Uses dynamic programming to create patches of minimized error.\n",
    "- Error is calculated iteratively by finding the error and number of switches.\n",
    "- values for minimum error is stored in each iteration.\n",
    "\n",
    "**recon**\n",
    "\n",
    "- Takes the array of time, value pairs and create a treashold points.\n",
    "- NaNs are removed and are not interpolated to have constant values.\n",
    "\n",
    "**Compress**\n",
    "\n",
    "- Creates an array of {time, value} values based on the current and previous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class piecewise_constant(encoder):\n",
    "    \"\"\"Represent the signal using a sequence of piecewise constant functions \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S, max_gap):\n",
    "        if type(S) != pd.Series:\n",
    "            raise 'encode expects pandas Series as input'\n",
    "        # Save the index and call the fit to update the model parameters.\n",
    "        self.index = S.index;\n",
    "        self.Sol = self.fit(S, max_gap)\n",
    "        print \"piecewise_constant model initialized\"\n",
    "    \n",
    "    def fit(self, S, max_gap):\n",
    "        print \"Fitting piecewise_constant\"\n",
    "        # Replace the nan values with 0\n",
    "        S.fillna(0)\n",
    "        \n",
    "        # Calculate the range of values.\n",
    "        # _range is a constant that is added to the error at each stop point\n",
    "        # Larger values will cause fewer switches.\n",
    "        _range=np.max(S) - np.min(S)\n",
    "        print 'range = ', _range\n",
    "\n",
    "        # Dynamic programming.\n",
    "        # An array that holds the best partition ending at each point of the sequence.\n",
    "        # Each element contains a best current value, a pointer to the last change in best\n",
    "        # solution so far and the total error of best solution so far.\n",
    "        Sol = [[]] * len(S)\n",
    "        for i in range(len(S)):\n",
    "            if i == 0:\n",
    "                Sol[i] = {'prev' : None, 'value' : S[0], 'error' : 0.0, 'switch_no' : 0}\n",
    "            # Sol is indexed by the location in the sequence S\n",
    "            # prev: the index of the last switch point\n",
    "            # value: current prediction value\n",
    "            # error: cumulative error to this point\n",
    "            # switch_no: number of switches so far.\n",
    "            else:\n",
    "                # Calculate the squared error with previous value.\n",
    "                err0 = Sol[i-1]['error'] + (Sol[i-1]['value'] - S[i]) ** 2\n",
    "                best, best_err, best_val = None, 1e20, S[i]\n",
    "                for j in xrange(np.max([0, i - max_gap]), i):\n",
    "                    \n",
    "                    # Calculate the mean and standard deviation of gap.\n",
    "                    _mean, _std = np.mean(S[j : i]), np.std(S[j : i])\n",
    "                    \n",
    "                    # Calculate the error\n",
    "                    err = _std * (i - j) + Sol[j]['error'] + _range\n",
    "                    \n",
    "                    # Compare and get the best params\n",
    "                    if err < best_err:\n",
    "                        best, best_val, best_err = j, _mean, err\n",
    "                # Store the best params.\n",
    "                Sol[i] = {'prev' : best, 'value' : best_val, 'error' : best_err,\\\n",
    "                        'switch_no': Sol[best]['switch_no'] + 1}\n",
    "        return Sol\n",
    "\n",
    "    def compress(self, S):\n",
    "        \"\"\"Compress the data.\"\"\"\n",
    "        # Initiallize the switch points.\n",
    "        Switch_points = []\n",
    "\n",
    "        # start from the end \n",
    "        i = len(self.Sol) - 1\n",
    "\n",
    "        while i > 0:\n",
    "            prev, value = self.Sol[i]['prev'], self.Sol[i]['value']\n",
    "            if self.Sol[prev]['value'] != value:\n",
    "                Switch_points.append({'time':S.index[prev],'value':value})\n",
    "            i = prev\n",
    "        self.compressed = Switch_points\n",
    "        return Switch_points\n",
    "\n",
    "    def recon(self, compressed = None, index = None):\n",
    "        \"\"\"Reconstructs the data from compressed data\n",
    "        \"\"\"\n",
    "        if type(index) == type(None):\n",
    "            index = self.index\n",
    "        Recon = pd.Series(index=index)\n",
    "\n",
    "        if type(compressed) == type(None):\n",
    "            compressed = self.compressed\n",
    "        for e in compressed:\n",
    "            time = e['time']\n",
    "            Recon[time] = e['value']\n",
    "\n",
    "        return Recon.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS: piecewise_linear\n",
    "\n",
    "**piecewise_linear is the class that performs the piecewise linear approximation on the data. **\n",
    "\n",
    "1. This class inherits encoder class.\n",
    "2. compress and recon are overridden from base class.\n",
    "3. internal method fit() is used to fit the data.\n",
    "\n",
    "\n",
    "**Constructor:**\n",
    "- Takes input as series.\n",
    "- Calls fit() to train the model.\n",
    "\n",
    "**fit**\n",
    "- Takes series of values and max_gap as input.\n",
    "- Uses dynamic programming to create patches of minimized error.\n",
    "- Error is calculated iteratively by finding the slope for each max_gap.\n",
    "- values for minimum error is stored in each iteration.\n",
    "\n",
    "**recon**\n",
    "\n",
    "- Linear interpolation is performed for reconstruction from the compresseed data.\n",
    "- numpy interpolation function is used.\n",
    "\n",
    "**Compress**\n",
    "\n",
    "- Iteratively stores the time and values for every changing slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class piecewise_linear(encoder):\n",
    "    \"\"\" \n",
    "    Represent the signal using a sequence of piecewise linear functions \n",
    "    \"\"\"\n",
    "    def __init__(self, S, max_gap):\n",
    "        if type(S) != pd.Series:\n",
    "            raise 'encode expects pandas Series as input'\n",
    "        # Save the index and call the fit to update the model parameters.\n",
    "        self.index = S.index\n",
    "        self.Sol = self.fit(S, max_gap)\n",
    "        print \"piecewise_linear model initialized\"\n",
    "    \n",
    "    # fit uses dynamic programming to find the best piecewise linear solution\n",
    "    # max_gap is the maximal extent of a single step.\n",
    "    # Reason for max_gap is that even if the error is small we want to correct\n",
    "    # it with some minimal frequence. \n",
    "    # Not quite a snapshot because the value will not necessarily change after \n",
    "    # max_gap is reached.\n",
    "    def fit(self, S, max_gap):\n",
    "        print \"Fitting piecewise_linear\"\n",
    "        # Replace the nan values with 0\n",
    "        S.fillna(0)\n",
    "        \n",
    "        # Calculate the range of values.\n",
    "        # _range is a constant that is added to the error at each stop point\n",
    "        # Larger values will cause fewer switches.\n",
    "        _range=np.max(S) - np.min(S)\n",
    "        print 'range = ', _range\n",
    "\n",
    "        # Dynamic programming.\n",
    "        # An array that holds the best partition ending at each point of the sequence.\n",
    "        # Each element contains a best current value, a pointer to the last change in best\n",
    "        # solution so far and the total error of best solution so far.\n",
    "        Sol = [[]] * len(S)\n",
    "        for i in range(len(S)):\n",
    "            if i == 0:\n",
    "                Sol[i]={'prev':None, 'value':S[0], 'error':0.0, 'switch_no':0, 'slope':0}\n",
    "            # Sol is indexed by the location in the sequence S\n",
    "            # prev: the index of the last switch point\n",
    "            # value: current prediction value\n",
    "            # error: cumulative error to this point\n",
    "            # slope: slope of th linear line at this point\n",
    "            else:\n",
    "                err0 = Sol[i-1]['error'] + (Sol[i-1]['value'] - S[i]) ** 2\n",
    "                best, best_err, best_val, best_slope = None, 1e20, S[i], 1e20\n",
    "                for j in xrange(np.max([0, i - max_gap]), i):\n",
    "                    \n",
    "                    # Calculate the slope\n",
    "                    _slope=(S[i] - S[j]) * 1.0 / (i - j)\n",
    "                    \n",
    "                    # Initialize the parameters.\n",
    "                    _val, _err = 0, 0\n",
    "                    for k in xrange(j, i):\n",
    "                        # Calculate the new value based on slope.\n",
    "                        _val = Sol[j]['value'] + _slope * (k - j)\n",
    "                        \n",
    "                        # Calculate the error.\n",
    "                        _err += (Sol[k]['value'] - _val) ** 2\n",
    "                    \n",
    "                    # Calculate the total error.\n",
    "                    # Need to understand why _range is addeed to error.\n",
    "                    err = _err * 1.0 / (i - j) + Sol[j]['error'] + _range\n",
    "                    _val = Sol[j]['value'] + _slope * (i - j)\n",
    "                    \n",
    "                    # Compare and get the best params.\n",
    "                    if err < best_err:\n",
    "                        best, best_val, best_err, best_slope, = j, _val,err,_slope\n",
    "\n",
    "                # Save the best params\n",
    "                Sol[i] = {'prev':best, 'value':best_val, 'error':best_err,\\\n",
    "                        'switch_no': Sol[best]['switch_no']+1, 'slope':best_slope}\n",
    "\n",
    "        # Return the fit parameters.\n",
    "        return Sol\n",
    "\n",
    "    def compress(self,S):\n",
    "        Switch_points = []\n",
    "        \n",
    "        # start from the end \n",
    "        i = len(self.Sol) - 1\n",
    "        while i > 0:\n",
    "            prev, slope, value, = self.Sol[i]['prev'], self.Sol[i]['slope'], self.Sol[i]['value']\n",
    "            if self.Sol[prev]['slope'] != slope:\n",
    "                Switch_points.append({'time' : S.index[prev], 'value' : value})\n",
    "            i = prev\n",
    "\n",
    "        # Save the compressed data and return the data.\n",
    "        self.compressed = Switch_points\n",
    "        return Switch_points\n",
    "\n",
    "    def recon(self,compressed=None, index=None):\n",
    "        if type(index)==type(None):\n",
    "            index = self.index\n",
    "\n",
    "        # Initialize the recon series.\n",
    "        Recon = pd.Series(index=index)\n",
    "\n",
    "        if type(compressed) == type(None):\n",
    "            compressed = self.compressed\n",
    "        for e in compressed:\n",
    "            time = e['time']\n",
    "            Recon[time] = e['value']\n",
    "        \n",
    "        # Interpolate the value using linear method.\n",
    "        Recon.interpolate(method=\"linear\", inplace=True)\n",
    "        return Recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: model\n",
    "\n",
    "**model is the API that calls piecewise linear / piecewise constant based on method,\n",
    "compresses, recostructs and calculates the error.**\n",
    "\n",
    "1. Get the encoder object by calling piecewise_constant / piecewise_linear\n",
    "2. Compress the data.\n",
    "3. Reconstruct the data.\n",
    "4. Calculate the compression error.\n",
    "5. Returns the compressed and reconstructed data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(pd_df, method, tolerance):\n",
    "    \"\"\"model calls either piecewise_constant or piecewise_linear based on the method.\n",
    "    It gets the appropriate encoder, compresses, reconstructs the data and calculates the error.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the values\n",
    "    S = pd_df['values']\n",
    "    \n",
    "    # Calculate the standard deviation of the values.\n",
    "    _std = np.std(S)\n",
    "    print \"Std dev is \", _std\n",
    "    # Call piecewise_constant / piecewise_linear API based on method and get the encoder.\n",
    "    if method == 'piecewise_constant':\n",
    "        encoder = piecewise_constant(S, tolerance)\n",
    "    elif(method == 'piecewise_linear'):\n",
    "        encoder = piecewise_linear(S, tolerance) # XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX chage it to linear\n",
    "    \n",
    "    # Compress the data as per the encoder.\n",
    "    C = encoder.compress(S)\n",
    "    \n",
    "    # Reconstruct the encoded data.\n",
    "    R = encoder.recon()\n",
    "\n",
    "    # Create the data frame of comparessed data.\n",
    "    compressed_df = pd.DataFrame(C)\n",
    "\n",
    "    # Calculate the error between compressed and original data.\n",
    "    error = encoder.compute_error(S, compressed=C)\n",
    "    \n",
    "    print 'error =', error, 'error/_std=',error/_std\n",
    "    \n",
    "    # Return the compressed and re constructed dataframes.\n",
    "    return [compressed_df, R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: runAnalysis\n",
    "\n",
    "**runAnalysis is the API that performs piecewise linear / piecewise constant analysis on a given teamplate.**\n",
    "\n",
    "1. Create a query based on template.\n",
    "2. Project timeseries and values.\n",
    "3. Convert the data into appropriate datatypes.\n",
    "4. Call the model on the data.\n",
    "5. Returns the compressed and reconstructed data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runAnalysis(room, stTime = None, enTime = None, templates = ['Zone Temperature'], method='piecewise_linear'):\n",
    "    \"\"\"runAnalysis is the API that performs piecewise linear / piecewise constant analysis on a given teamplate.\"\"\"\n",
    "    \n",
    "    # Initialize the data frames and templates to plot.\n",
    "    dfs = []\n",
    "    plotTemplates=[]\n",
    "    dfs_compressed = []\n",
    "    \n",
    "    # Run the analysis for each template upto the template count.\n",
    "    for t in templates:\n",
    "        try:\n",
    "            # get the signal dataframe for specified room and signals\n",
    "            if type(room) is pd.core.frame.DataFrame:\n",
    "                dataDF = room\n",
    "            elif type(room) is str:\n",
    "                dataDF = get_signal_dataframe(room, [t], mean_type=None, use_weather_data=False)\n",
    "\n",
    "            if not stTime is None:\n",
    "                stTime = pd.to_datetime(stTime)\n",
    "                dataDF = dataDF[dataDF.time >= stTime]\n",
    "            if not enTime is None:\n",
    "                enTime = pd.to_datetime(enTime)\n",
    "                dataDF = dataDF[dataDF.time <= enTime]\n",
    "\n",
    "            if len(dataDF) < 50:\n",
    "                print \"small data frame. length = \", len(dataDF)\n",
    "\n",
    "            # rename the index in sequence\n",
    "            dataDF.index = range(len(dataDF))\n",
    "\n",
    "            # Rename the columns names from time, <template> to timesries to values\n",
    "            dataDF = dataDF[['time', t]].rename(columns={'time' : 'timeseries', t : 'values'})\n",
    "\n",
    "            # Run the model and get the compressed dataframe and reconstructed data frame.\n",
    "            [compressedDF, reconDF] = model(dataDF, method, tolerance = 10)\n",
    "            \n",
    "            # Append the template and output data frames of model.\n",
    "            plotTemplates.append(t)\n",
    "            dfs.extend([dataDF, reconDF])\n",
    "            dfs_compressed.append(compressedDF)\n",
    "        except:\n",
    "            print \"Exception for template: \", t\n",
    "\n",
    "    # Return the templates and dataframes.\n",
    "    return [dfs, plotTemplates, dfs_compressed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: CompressWithPCA\n",
    "\n",
    "**CompressWithPCA is the API that performs PCA on the data frame and plots the original and reconstructed for template.**\n",
    "\n",
    "1. Takes the data frame as input\n",
    "2. Performs PCA, transforms and reconstructs the data frame with the number of components specified\n",
    "3. filters the data frame based on date range\n",
    "4. selects the required template from original and reconstructed\n",
    "5. returns the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CompressWithPCA(dataDF, stTime, enTime, template='Zone Temperature', n_components = 9):\n",
    "    # Fill the nan values with its previous values\n",
    "    dataDF.fillna(method='bfill', inplace = True)\n",
    "\n",
    "    # Remove first 2 columns. first 2 colunms are date and location.\n",
    "    original_df = dataDF.iloc[:, 2:]\n",
    "\n",
    "    pca = PCA(n_components = n_components).fit(original_df)\n",
    "    transformed = pca.transform(original_df)\n",
    "    reconstructed = pca.inverse_transform(transformed)\n",
    "    reconstructed_df = pd.concat([dataDF.iloc[:,0:2], pd.DataFrame(reconstructed, columns=dataDF.columns[2:])], axis=1)\n",
    "\n",
    "    if not stTime is None:\n",
    "        stTime = pd.to_datetime(stTime)\n",
    "        dataDF = dataDF[(dataDF.time >= stTime)]\n",
    "    if not enTime is None:\n",
    "        enTime = pd.to_datetime(enTime)\n",
    "        dataDF = dataDF[(dataDF.time <= enTime)]\n",
    "    \n",
    "    df_orig = dataDF.rename(columns={'time':'timeseries', template:'values'})\n",
    "    df_reconstruct = reconstructed_df[(reconstructed_df.time >= stTime) & (reconstructed_df.time <= enTime)][template]\n",
    "    return [[df_orig, df_reconstruct], [template], [transformed]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API: run_length_encoding\n",
    "\n",
    "**run_length_encoding is the API that performs run length encoding on the signals specified.**\n",
    "\n",
    "1. Inputs: room / dataframe, signals to compress, stTime, enTime, tolerance, plot template\n",
    "2. Performs run length encoding on the signals\n",
    "3. filters the data frame based on date range\n",
    "4. returns the original, compressed and reconstructed dataframes, data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_bucket(se1, se2, tolerance):\n",
    "    mask = (se1 - se2).abs() > tolerance\n",
    "    val = False if mask.sum() else True\n",
    "    return val\n",
    "\n",
    "def run_length_encoding(room, signals = None, stTime = None, enTime = None, tolerance = None, template='Zone Temperature'):\n",
    "    \n",
    "    # get the signal dataframe for specified room and signals\n",
    "    if type(room) is pd.core.frame.DataFrame:\n",
    "        dataDF = room\n",
    "    elif type(room) is str:\n",
    "        dataDF = get_signal_dataframe(room, signals, mean_type=None, use_weather_data=False)\n",
    "\n",
    "    # filter the data for the required time\n",
    "    if not stTime is None:\n",
    "        stTime = pd.to_datetime(stTime)\n",
    "        dataDF = dataDF[dataDF.time >= stTime]\n",
    "    if not enTime is None:\n",
    "        enTime = pd.to_datetime(enTime)\n",
    "        dataDF = dataDF[dataDF.time <= enTime]\n",
    "\n",
    "    # rename the index in sequence\n",
    "    dataDF.index = range(len(dataDF))\n",
    "\n",
    "    # calculate the tolerance\n",
    "    if tolerance is None:\n",
    "        tolerance = [pd.to_timedelta(\"15 minutes\")] + [0] * (len(dataDF.columns) - 1)\n",
    "\n",
    "    # initial reference is the first row\n",
    "    count, num_rows = 1, len(dataDF)\n",
    "    reference1, reference2 = dataDF.iloc[0, :].copy(), dataDF.iloc[0, :].copy()\n",
    "    \n",
    "    # initialize the empry compresed data frame\n",
    "    compressed_df = pd.DataFrame(columns=dataDF.columns.tolist() + [\"count\"])\n",
    "    \n",
    "    # run the loop for every row\n",
    "    for i in range(1, num_rows):\n",
    "        se = dataDF.iloc[i, :]\n",
    "        if not same_bucket(reference2, se, tolerance):\n",
    "            reference1['count'] = count\n",
    "            compressed_df = compressed_df.append(reference1)\n",
    "            reference1, reference2 = se.copy(), se.copy()\n",
    "            count = 1\n",
    "        else:\n",
    "            count += 1\n",
    "            reference2['time'] = se['time']\n",
    "    reference1['count'] = count\n",
    "    compressed_df = compressed_df.append(reference1)\n",
    "\n",
    "    recon_df = pd.DataFrame(dataDF.time)\n",
    "    recon_df = recon_df.merge(compressed_df, how='left').fillna(method='ffill')\n",
    "\n",
    "    df_orig = dataDF.rename(columns={'time':'timeseries', template:'values'})\n",
    "    return [[df_orig, recon_df[template]], [template], [compressed_df]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Mean Clustering on the Room Data\n",
    "\n",
    "**What**\n",
    "- Perform KMeans on the data to group them into various clusters.\n",
    "\n",
    "**Why**\n",
    "- Compression using PCA on clusters is lot better then the whole data.\n",
    "\n",
    "**Procedure**\n",
    "1. Define kmeans to provide the cluster and its centers\n",
    "2. Calculate RMS Error based on the Predictions and evaluate the clustering Effectiveness\n",
    "3. Evaluate the right k value plotting the RMS error on a elbow curve\n",
    "4. Based on the elbow curve below, k = 4 is the most ideal\n",
    "5. Now using k=4 Run the k means cluster again and determine the cluster each row in the dataframe belongs to\n",
    "6. Using Each Cluster , now run them through PCA for 1 of the signals \"Zone Temperature\" to understand the patterns on each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performKmeans(n_clusters, dataDF):\n",
    "    # Initialize the kmeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "    # classify the data in to clusters\n",
    "    predicted = kmeans.fit_predict(dataDF)\n",
    "    \n",
    "    return predicted, kmeans.cluster_centers_\n",
    "\n",
    "def calculateRMSE(original_df, labels, centers):\n",
    "    labels_df = pd.DataFrame(labels, columns=['cluster'])\n",
    "    clustered_df = labels_df.merge(pd.DataFrame(centers), left_on='cluster', right_index=True, how='left')\n",
    "    return mean_squared_error(original_df, clustered_df.iloc[:, 1:])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
